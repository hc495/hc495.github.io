- title: "Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis"
  authors:
    - name: Haolin
    - name: me
    - name: n-inoue
  year: 2025
  basic_url: "https://arxiv.org/abs/2509.24164"
  pages: 45
  urls:
    - url: "https://arxiv.org/pdf/2509.24164"
      label: "PDF"
    - url: "https://arxiv.org/abs/2509.24164"
      label: "arXiv"
  abstract: "We investigate the mechanistic underpinnings of in-context learning (ICL) in large language models by reconciling two dominant perspectives: the component-level analysis of attention heads and the holistic decomposition of ICL into Task Recognition (TR) and Task Learning (TL). We propose a novel framework based on Task Subspace Logit Attribution (TSLA) to identify attention heads specialized in TR and TL, and demonstrate their distinct yet complementary roles. Through correlation analysis, ablation studies, and input perturbations, we show that the identified TR and TL heads independently and effectively capture the TR and TL components of ICL. Using steering experiments with geometric analysis of hidden states, we reveal that TR heads promote task recognition by aligning hidden states with the task subspace, while TL heads rotate hidden states within the subspace toward the correct label to facilitate prediction. We further show how previous findings on ICL mechanisms, including induction heads and task vectors, can be reconciled with our attention-head-level analysis of the TR-TL decomposition. Our framework thus provides a unified and interpretable account of how large language models execute ICL across diverse tasks and settings."
  id: "arxiv9"
  bibtex: |
    @article{yang2025localizingtaskrecognitiontask,<br>
    &nbsp;&nbsp;&nbsp;&nbsp;title={Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;author={Yang, Haolin and Cho, Hakaze and Inoue, Naoya},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2509.24164},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;year={2025}<br>
    }


- title: "Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight"
  authors:
    - name: Haolin
    - name: me
    - name: "Kaize Ding"
      url: "https://kaize0409.github.io/"
    - name: n-inoue
  year: 2025
  basic_url: "https://arxiv.org/abs/2509.24169"
  pages: 48
  urls:
    - url: "https://arxiv.org/pdf/2509.24169"
      label: "PDF"
    - url: "https://arxiv.org/abs/2509.24169"
      label: "arXiv"
  abstract: "Large Language Models (LLMs) can perform new tasks from in-context demonstrations, a phenomenon known as in-context learning (ICL). Recent work suggests that these demonstrations are compressed into task vectors (TVs), compact task representations that LLMs exploit for predictions. However, prior studies typically extract TVs from model outputs or hidden states using cumbersome and opaque methods, and they rarely elucidate the mechanisms by which TVs influence computation. In this work, we address both limitations. First, we propose directly training Learned Task Vectors (LTVs), which surpass extracted TVs in accuracy and exhibit superior flexibility-acting effectively at arbitrary layers, positions, and even with ICL prompts. Second, through systematic analysis, we investigate the mechanistic role of TVs, showing that at the low level they steer predictions primarily through attention-head OV circuits, with a small subset of 'key heads' most decisive. At a higher level, we find that despite Transformer nonlinearities, TV propagation is largely linear: early TVs are rotated toward task-relevant subspaces to improve logits of relevant labels, while later TVs are predominantly scaled in magnitude. Taken together, LTVs not only provide a practical approach for obtaining effective TVs but also offer a principled lens into the mechanistic foundations of ICL."
  id: "arxiv8"
  bibtex: |
    @article{yang2025taskvectorslearnedextracted,<br>
    &nbsp;&nbsp;&nbsp;&nbsp;title={Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;author={Yang, Haolin and Cho, Hakaze and Ding, Kaize and Inoue, Naoya},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2509.24169},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;year={2025}<br>
    }

- title: "Binary Autoencoder for Mechanistic Interpretability of Large Language Models"
  authors:
    - name: me
    - name: Haolin
    - name: Brian M. Kurkoski
      url: "https://www.jaist.ac.jp/is/labs/bits/brian"
    - name: n-inoue
  year: 2025
  basic_url: "https://arxiv.org/abs/2509.20997"
  pages: 36
  urls:
    - url: "https://arxiv.org/pdf/2509.20997"
      label: "PDF"
    - url: "https://arxiv.org/abs/2509.20997"
      label: "arXiv"
  abstract: "Existing works are dedicated to untangling atomized numerical components (features) from the hidden states of Large Language Models (LLMs) for interpreting their mechanism. However, they typically rely on autoencoders constrained by some implicit training-time regularization on single training instances (i.e.,  normalization, top-k function, etc.), without an explicit guarantee of global sparsity among instances, causing a large amount of dense (simultaneously inactive) features, harming the feature sparsity and atomization. In this paper, we propose a novel autoencoder variant that enforces minimal entropy on minibatches of hidden activations, thereby promoting feature independence and sparsity across instances. For efficient entropy calculation, we discretize the hidden activations to 1-bit via a step function and apply gradient estimation to enable backpropagation, so that we term it as Binary Autoencoder (BAE) and empirically demonstrate two major applications: (1) Feature set entropy calculation. Entropy can be reliably estimated on binary hidden activations, which we empirically evaluate and leverage to characterize the inference dynamics of LLMs and In-context Learning. (2) Feature untangling. Similar to typical methods, BAE can extract atomized features from LLM's hidden states. To robustly evaluate such feature extraction capability, we refine traditional feature-interpretation methods to avoid unreliable handling of numerical tokens, and show that BAE avoids dense features while producing the largest number of interpretable ones among baselines, which confirms the effectiveness of BAE serving as a feature extractor."
  id: "arxiv7"
  bibtex: |
    @article{cho2025binary,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={Binary Autoencoder for Mechanistic Interpretability of Large Language Models},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Cho, Hakaze and Yang, Haolin and Kurkoski, Brian M. and Inoue, Naoya},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2509.20997},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2025}<br>
    }

- title: "Mechanism of Task-oriented Information Removal in In-context Learning"
  authors:
    - name: me
    - name: Haolin
    - name: Gouki
    - name: n-inoue
  year: 2025
  basic_url: "https://arxiv.org/abs/2509.21012"
  pages: 87
  urls:
    - url: "https://arxiv.org/pdf/2509.21012"
      label: "PDF"
    - url: "https://arxiv.org/abs/2509.21012"
      label: "arXiv"
  abstract: "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads."
  id: "arxiv6"
  bibtex: |
    @article{cho2025mechanism,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={Mechanism of Task-oriented Information Removal in In-context Learning},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Cho, Hakaze and Yang, Haolin and Minegishi, Gouki and Inoue, Naoya},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2509.21012},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2025}<br>
    }

- title: "Measuring Intrinsic Dimension of Token Embeddings"
  authors:
    - name: "Takuya Kataiwa"
    - name: me
    - name: "Tetsushi Ohki"
      url: "https://sec.inf.shizuoka.ac.jp/people/ohki/"
  year: 2025
  basic_url: "https://arxiv.org/abs/2503.02142"
  pages: 6
  urls:
    - url: "https://arxiv.org/pdf/2503.02142"
      label: "PDF"
    - url: "https://arxiv.org/abs/2503.02142"
      label: "arXiv"
  abstract: "In this study, we measure the Intrinsic Dimension (ID) of token embedding to estimate the intrinsic dimensions of the manifolds spanned by the representations, so as to evaluate their redundancy quantitatively compared to their extrinsic dimensionality. In detail, (1) we estimate the ID of token embeddings in small-scale language models and also modern large language models, finding that the embedding spaces often reside on lower-dimensional manifolds compared to their extrinsic dimensionality; (2) we measure the ID across various model sizes and observe an increase in redundancy rates as the model scale grows; (3) we measure the dynamics of IDs during the training process, and find a rapid ID drop in the early stages of training. Moreover, (4) when LoRA is applied to the embedding layers, we observe a sudden drop in perplexity around the estimated IDs, suggesting that the ID can serve as a useful guideline for LoRA application."
  id: "arxiv1"
  bibtex: |
    @article{kataiwa2025measuring,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={Measuring Intrinsic Dimension of Token Embeddings},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Kataiwa, Takuya and Cho, Hakaze and Ohki, Tetsushi},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2503.02142},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2025}<br>
    }

- title: "Affinity and Diversity: A Unified Metric for Demonstration Selection via Internal Representations"
  authors:
    - name: "Mariko Kato"
    - name: me
    - name: "Yoshihiro Sakai"
    - name: n-inoue
  year: 2025
  basic_url: "https://arxiv.org/abs/2502.14380"
  pages: 8
  urls:
    - url: "https://arxiv.org/abs/2502.14380"
      label: "PDF"
    - url: "https://arxiv.org/abs/2502.1438"
      label: "arXiv"
  abstract: "The performance of In-Context Learning (ICL) is highly sensitive to the selected demonstrations. Existing approaches to demonstration selection optimize different objectives, yielding inconsistent results. To address this, we propose a unified metric--affinity and diversity--that leverages ICL model's internal representations. Our experiments show that both affinity and diversity strongly correlate with test accuracies, indicating their effectiveness for demonstration selection. Moreover, we show that our proposed metrics align well with various previous works to unify the inconsistency."
  id: "arxiv2"
  bibtex: |
    @article{kato2025affinity,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={Affinity and Diversity: A Unified Metric for Demonstration Selection via Internal Representations},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Kato, Mariko and Cho, Hakaze and Sakai, Yoshihiro and Inoue, Naoya},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2502.14380},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2025}<br>
    }

- title: "StaICC: Standardized Evaluation for Classification Task in In-context Learning"
  authors:
    - name: me
    - name: n-inoue
  year: 2025
  basic_url: "https://arxiv.org/abs/2501.15708"
  pages: 20
  urls:
    - url: "https://arxiv.org/abs/2501.15708"
      label: "PDF"
    - url: "https://arxiv.org/abs/2501.15708"
      label: "arXiv"
    - url: "https://github.com/hc495/StaICC"
      label: "Github"
    - url: "https://pypi.org/project/StaICC/"
      label: "PyPI"
  abstract: "Classification tasks are widely investigated in the In-Context Learning (ICL) paradigm. However, current efforts are evaluated on disjoint benchmarks and settings, while their performances are significantly influenced by some trivial variables, such as prompt templates, data sampling, instructions, etc., which leads to significant inconsistencies in the results reported across various literature, preventing fair comparison or meta-analysis across different papers. Therefore, this paper proposes a standardized and easy-to-use evaluation toolkit (StaICC) for in-context classification. Including, for the normal classification task, we provide StaICC-Normal, selecting 10 widely used datasets, and generating prompts with a fixed form, to mitigate the variance among the experiment implementations. To enrich the usage of our benchmark, we also provide a sub-benchmark StaICC-Diag for diagnosing ICL from several aspects, aiming for a more robust inference processing."
  id: "arxiv3"
  bibtex: |
    @article{cho2025staicc,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={StaICC: Standardized Evaluation for Classification Task in In-context Learning},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Cho, Hakaze and Inoue, Naoya},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2501.15708},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2025}<br>
    }

- title: "NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning"
  authors:
    - name: "Yufeng Zhao"
    - name: "Yoshihiro Sakai"
    - name: n-inoue
  year: 2024
  basic_url: "https://arxiv.org/abs/2402.05515"
  pages: 20
  urls:
    - url: "https://arxiv.org/abs/2402.05515"
      label: "PDF"
    - url: "https://arxiv.org/abs/2402.05515"
      label: "arXiv"
    - url: "https://github.com/hc495/NoisyICL"
      label: "Github"
  abstract: "In-Context Learning (ICL) is suffering from unsatisfactory performance and under-calibration due to high prior bias and unfaithful confidence. Some previous works fine-tuned language models for better ICL performance with enormous datasets and computing costs. In this paper, we propose NoisyICL, simply perturbing the model parameters by random noises to strive for better performance and calibration. Our experiments on two models and 12 downstream datasets show that NoisyICL can help ICL produce more accurate predictions. Our further analysis indicates that NoisyICL enables the model to provide more fair predictions, and also with more faithful confidence. Therefore, we believe that NoisyICL is an effective calibration of ICL. Our experimental code is uploaded to Github."
  id: "arxiv4"
  bibtex: |
    @article{zhao2024noisyicl,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Zhao, Yufeng and Sakai, Yoshihiro and Inoue, Naoya},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2402.05515},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2024}<br>
    }

