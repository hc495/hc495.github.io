- title: "Mechanism of Task-oriented Information Removal in In-context Learning"
  authors:
    - name: me
    - name: Haolin
    - name: Gouki
    - name: n-inoue
  year: 2025
  venue: iclr
  basic_url: "https://openreview.net/forum?id=VAv1rrPR1A"
  pages: 87
  urls:
    - url: "https://openreview.net/forum?id=VAv1rrPR1A"
      label: "OpenReview"
    - url: "https://openreview.net/pdf?id=VAv1rrPR1A"
      label: "PDF"
    - url: "https://arxiv.org/abs/2509.21012"
      label: "arXiv"
    - url: "https://github.com/hc495/Verb_subspace"
      label: "Github"
  abstract: "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads."
  id: "arxiv6"
  bibtex: |
    @article{cho2025mechanism,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={Mechanism of Task-oriented Information Removal in In-context Learning},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Cho, Hakaze and Yang, Haolin and Minegishi, Gouki and Inoue, Naoya},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;booktitle={The Fourteenth International Conference on Learning Representations},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2026},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;url={https://openreview.net/forum?id=VAv1rrPR1A}<br>
    }

- title: "Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis"
  authors:
    - name: Haolin
    - name: me
    - name: n-inoue
  year: 2025
  venue: iclr
  basic_url: "https://arxiv.org/abs/2509.24164"
  pages: 45
  urls:
    - url: "https://arxiv.org/pdf/2509.24164"
      label: "PDF"
    - url: "https://arxiv.org/abs/2509.24164"
      label: "arXiv"
  abstract: "We investigate the mechanistic underpinnings of in-context learning (ICL) in large language models by reconciling two dominant perspectives: the component-level analysis of attention heads and the holistic decomposition of ICL into Task Recognition (TR) and Task Learning (TL). We propose a novel framework based on Task Subspace Logit Attribution (TSLA) to identify attention heads specialized in TR and TL, and demonstrate their distinct yet complementary roles. Through correlation analysis, ablation studies, and input perturbations, we show that the identified TR and TL heads independently and effectively capture the TR and TL components of ICL. Using steering experiments with geometric analysis of hidden states, we reveal that TR heads promote task recognition by aligning hidden states with the task subspace, while TL heads rotate hidden states within the subspace toward the correct label to facilitate prediction. We further show how previous findings on ICL mechanisms, including induction heads and task vectors, can be reconciled with our attention-head-level analysis of the TR-TL decomposition. Our framework thus provides a unified and interpretable account of how large language models execute ICL across diverse tasks and settings."
  id: "arxiv9"
  bibtex: |
    @article{yang2025localizingtaskrecognitiontask,<br>
    &nbsp;&nbsp;&nbsp;&nbsp;title={Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;author={Yang, Haolin and Cho, Hakaze and Inoue, Naoya},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2509.24164},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;year={2025}<br>
    }


- title: "Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight"
  authors:
    - name: Haolin
    - name: me
    - name: "Kaize Ding"
      url: "https://kaize0409.github.io/"
    - name: n-inoue
  year: 2025
  venue: iclr
  basic_url: "https://arxiv.org/abs/2509.24169"
  pages: 48
  urls:
    - url: "https://arxiv.org/pdf/2509.24169"
      label: "PDF"
    - url: "https://arxiv.org/abs/2509.24169"
      label: "arXiv"
  abstract: "Large Language Models (LLMs) can perform new tasks from in-context demonstrations, a phenomenon known as in-context learning (ICL). Recent work suggests that these demonstrations are compressed into task vectors (TVs), compact task representations that LLMs exploit for predictions. However, prior studies typically extract TVs from model outputs or hidden states using cumbersome and opaque methods, and they rarely elucidate the mechanisms by which TVs influence computation. In this work, we address both limitations. First, we propose directly training Learned Task Vectors (LTVs), which surpass extracted TVs in accuracy and exhibit superior flexibility-acting effectively at arbitrary layers, positions, and even with ICL prompts. Second, through systematic analysis, we investigate the mechanistic role of TVs, showing that at the low level they steer predictions primarily through attention-head OV circuits, with a small subset of 'key heads' most decisive. At a higher level, we find that despite Transformer nonlinearities, TV propagation is largely linear: early TVs are rotated toward task-relevant subspaces to improve logits of relevant labels, while later TVs are predominantly scaled in magnitude. Taken together, LTVs not only provide a practical approach for obtaining effective TVs but also offer a principled lens into the mechanistic foundations of ICL."
  id: "arxiv8"
  bibtex: |
    @article{yang2025taskvectorslearnedextracted,<br>
    &nbsp;&nbsp;&nbsp;&nbsp;title={Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;author={Yang, Haolin and Cho, Hakaze and Ding, Kaize and Inoue, Naoya},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2509.24169},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;year={2025}<br>
    }

- title: "Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning"
  authors:
    - name: Haolin
    - name: me
    - name: "Yiqiao Zhong"
    - name: n-inoue
  venue: NeurIPS
  year: 2025
  basic_url: "https://openreview.net/forum?id=FIfjDqjV0B"
  pages: 52
  urls:
    - url: "https://openreview.net/forum?id=FIfjDqjV0B"
      label: "OpenReview"
    - url: "https://openreview.net/pdf?id=FIfjDqjV0B"
      label: "PDF"
    - url: "https://arxiv.org/abs/2505.18752"
      label: "arXiv"
    - url: "https://neurips.cc/virtual/2025/poster/119047"
      label: "Poster"
    - url: "https://github.com/HLYang2001/ICL_Hidden_Geometry"
      label: "Github"
  abstract: "The unusual properties of in-context learning (ICL) have prompted investigations into the internal mechanisms of large language models. Prior work typically focuses on either special attention heads or task vectors at specific layers, but lacks a unified framework linking these components to the evolution of hidden states across layers that ultimately produce the model's output. In this paper, we propose such a framework for ICL in classification tasks by analyzing two geometric factors that govern performance: the separability and alignment of query hidden states. A fine-grained analysis of layer-wise dynamics reveals a striking two-stage mechanism: separability emerges in early layers, while alignment develops in later layers. Ablation studies further show that Previous Token Heads drive separability, while Induction Heads and task vectors enhance alignment. Our findings thus bridge the gap between attention heads and task vectors, offering a unified account of ICL's underlying mechanisms."
  id: "nips1"
  bibtex: |
    @inproceedings{yang2025unifying,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Yang, Haolin and Cho, Hakaze and Zhong, Yiqiao and Inoue, Naoya},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2025},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;url={https://openreview.net/forum?id=FIfjDqjV0B}<br>
    }

- title: "Mechanistic Fine-tuning for In-context Learning"
  authors:
    - name: me
    - name: "Peng Luo"
    - name: "Mariko Kato"
    - name: "Rin Kaenbyou"
    - name: n-inoue
  venue: BlackBox
  year: 2025
  basic_url: "https://aclanthology.org/2025.blackboxnlp-1.21/"
  pages: 28
  urls:
    - url: "https://aclanthology.org/2025.blackboxnlp-1.21/"
      label: "ACL Anthology"
    - url: "https://arxiv.org/pdf/2505.14233"
      label: "PDF"
    - url: "https://arxiv.org/abs/2505.14233"
      label: "arXiv"
    - url: "https://github.com/hc495/ICL_head_tuning"
      label: "Github"
  abstract: "In-context Learning (ICL) utilizes structured demonstration-query inputs to induce few-shot learning on Language Models (LMs), which are not originally pre-trained on ICL-style data. To bridge the gap between ICL and pre-training, some approaches fine-tune LMs on large ICL-style datasets by an end-to-end paradigm with massive computational costs. To reduce such costs, in this paper, we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous findings on the inner mechanism of ICL, building training objectives on the attention scores instead of the final outputs, to force the attention scores to focus on the correct label tokens presented in the context and mitigate attention scores from the wrong label tokens. Our experiments on 9 modern LMs and 8 datasets empirically find that ABFT outperforms in performance, robustness, unbiasedness, and efficiency, with only around 0.01% data cost compared to the previous methods. Moreover, our subsequent analysis finds that the end-to-end training objective contains the ABFT objective, suggesting the implicit bias of ICL-style data to the emergence of induction heads. Our work demonstrates the possibility of controlling specific module sequences within LMs to improve their behavior, opening up the future application of mechanistic interpretability."
  id: "blackbox1"
  bibtex: |
    @inproceedings{cho2025mechanistic,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={Mechanistic Fine-tuning for In-context Learning},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Cho, Hakaze and Luo, Peng and Kato, Mariko and Kaenbyou, Rin and Inoue, Naoya},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;booktitle = "Proceedings of the 8th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP",<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2025},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;url={https://arxiv.org/abs/2505.14233}<br>
    }
  other: "Workshop at EMNLP 2025"

- title: "Revisiting In-context Learning Inference Circuit in Large Language Models"
  authors:
    - name: me
    - name: "Mariko Kato"
    - name: "Yoshihiro Sakai"
    - name: n-inoue
  venue: iclr
  year: 2025
  basic_url: "https://openreview.net/forum?id=xizpnYNvQq"
  pages: 37
  urls:
    - url: "https://openreview.net/forum?id=xizpnYNvQq"
      label: "OpenReview"
    - url: "https://openreview.net/pdf?id=xizpnYNvQq"
      label: "PDF"
    - url: "https://arxiv.org/abs/2410.04468"
      label: "arXiv"
    - url: "https://github.com/hc495/ICL_Circuit"
      label: "Github"
    - url: "https://iclr.cc/virtual/2025/poster/27767"
      label: "Poster"
    - url: "https://openreview.net/forum?id=xizpnYNvQq"
      label: "Review"
  abstract: "In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to explain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Input Text Encode: LMs encode every input text (in the demonstrations and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge the encoded representations of demonstrations with their corresponding label tokens to produce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations of demonstrations similar to the query representation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a certain extent and decode them into predicted labels. Through careful measurements, the proposed inference circuit successfully captures and unifies many fragmented phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. Moreover, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mechanism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit."
  id: "iclr1"
  bibtex: |
    @inproceedings{cho2025revisiting,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={Revisiting In-context Learning Inference Circuit in Large Language Models},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Hakaze Cho and Mariko Kato and Yoshihiro Sakai and Naoya Inoue},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;booktitle={The Thirteenth International Conference on Learning Representations},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2025},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;url={https://openreview.net/forum?id=xizpnYNvQq}<br>
    }

- title: "Token-based Decision Criteria Are Suboptimal in In-context Learning"
  authors:
    - name: me
    - name: "Yoshihiro Sakai"
    - name: "Mariko Kato"
    - name: "Kenshiro Tanaka"
    - name: "Akira Ishii"
    - name: n-inoue
  venue: naacl
  year: 2025
  basic_url: "https://aclanthology.org/2025.naacl-long.278/"
  pages: 24
  urls:
    - url: "https://aclanthology.org/2025.naacl-long.278/"
      label: "ACL Anthology"
    - url: "https://aclanthology.org/2025.naacl-long.278.pdf"
      label: "PDF"
    - url: "https://arxiv.org/abs/2406.16535"
      label: "arXiv"
    - url: "https://github.com/hc495/Hidden_Calibration"
      label: "Github"
    - url: "https://drive.google.com/file/d/1bD4cWT50GeW1DxABqoLPV7Ljbw12BgrE/view"
      label: "Poster"
    - url: "/reviews/hidden_calibration"
      label: "Review"
  abstract: "In-Context Learning (ICL) typically utilizes classification criteria from output probabilities of manually selected label tokens. However, we argue that such token-based classification criteria lead to suboptimal decision boundaries, despite delicate calibrations through translation and constrained rotation applied. To address this problem, we propose Hidden Calibration, which renounces token probabilities and uses the nearest centroid classifier on the LMâ€™s last hidden states. In detail, we assign the label of the nearest centroid previously estimated from a calibration set to the test sample as the predicted label. Our experiments on 6 models and 10 classification datasets indicate that Hidden Calibration consistently outperforms current token-based baselines by about 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis demonstrates that Hidden Calibration finds better classification criteria with less inter-class overlap, and LMs provide linearly separable intra-class clusters with the help of demonstrations, which supports Hidden Calibration and gives new insights into the principle of ICL. Our official code implementation can be found at https://github.com/hc495/Hidden_Calibration."
  id: "naacl1"
  bibtex: |
    @inproceedings{cho2025token,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={Token-based Decision Criteria Are Suboptimal in In-context Learning},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Hakaze Cho and Yoshihiro Sakai and Mariko Kato and Kenshiro Tanaka and Akira Ishii and Naoya Inoue},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;booktitle={Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2025},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;url={https://aclanthology.org/2025.naacl-long.278/}<br>
    }

- title: "Understanding Token Probability Encoding in Output Embeddings"
  authors:
    - name: me
    - name: "Yoshihiro Sakai"
    - name: "Kenshiro Tanaka"
    - name: "Mariko Kato"
    - name: n-inoue
  venue: coling
  year: 2025
  basic_url: "https://aclanthology.org/2025.coling-main.708/"
  pages: 16
  urls:
    - url: "https://aclanthology.org/2025.coling-main.708/"
      label: "ACL Anthology"
    - url: "https://aclanthology.org/2025.coling-main.708.pdf"
      label: "PDF"
    - url: "https://arxiv.org/abs/2406.01468"
      label: "arXiv"
    - url: "https://drive.google.com/file/d/1U11m_Qonq_F9d3GDD04b3yYO0KzFSGhJ/view"
      label: "Poster"
  abstract: "In this paper, we investigate the output token probability information in the output embedding of language models. We find an approximate common log-linear encoding of output token probabilities within the output embedding vectors and empirically demonstrate that it is accurate and sparse. As a causality examination, we steer the encoding in output embedding to modify the output probability distribution accurately. Moreover, the sparsity we find in output probability encoding suggests that a large number of dimensions in the output embedding do not contribute to causal language modeling. Therefore, we attempt to delete the output-unrelated dimensions and find more than 30% of the dimensions can be deleted without significant movement in output distribution and sequence generation. Additionally, in the pre-training dynamics of language models, we find that the output embeddings capture the corpus token frequency information in early steps, even before an obvious convergence of parameters starts."
  id: "coling1"
  bibtex: |
    @inproceedings{cho2025understanding,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={Understanding Token Probability Encoding in Output Embeddings},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Hakaze Cho and Yoshihiro Sakai and Kenshiro Tanaka and Mariko Kato and Naoya Inoue},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;booktitle={Proceedings of the 31st International Conference on Computational Linguistics},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2025},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;url={https://aclanthology.org/2025.coling-main.708/}<br>
    }

- title: "Find-the-Common: A Benchmark for Explaining Visual Patterns from Images"
  authors:
    - name: "Yuting Shi"
    - name: n-inoue
    - name: "Houjing Wei"
    - name: "Yufeng Zhao"
    - name: "Tao Jin"
  venue: lrec
  year: 2024
  basic_url: "https://aclanthology.org/2024.lrec-main.642/"
  pages: 7
  urls:
    - url: "https://aclanthology.org/2024.lrec-main.642/"
      label: "ACL Anthology"
    - url: "https://aclanthology.org/2024.lrec-main.642.pdf"
      label: "PDF"
  abstract: "Recent advances in Instruction-fine-tuned Vision and Language Models (IVLMs), such as GPT-4V and InstructBLIP, have prompted some studies have started an in-depth analysis of the reasoning capabilities of IVLMs. However, Inductive Visual Reasoning, a vital skill for text-image understanding, remains underexplored due to the absence of benchmarks. In this paper, we introduce Find-the-Common (FTC): a new vision and language task for Inductive Visual Reasoning. In this task, models are required to identify an answer that explains the common attributes across visual scenes. We create a new dataset for the FTC and assess the performance of several contemporary approaches including Image-Based Reasoning, Text-Based Reasoning, and Image-Text-Based Reasoning with various models. Extensive experiments show that even state-of-the-art models like GPT-4V can only archive with 48% accuracy on the FTC, for which, the FTC is a new challenge for the visual reasoning research community. Our dataset has been released and is available online: https://github.com/SSSSSeki/Find-the-common."
  id: "lrec1"
  bibtex: |
    @inproceedings{shi2024find,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={Find-the-Common: A Benchmark for Explaining Visual Patterns from Images},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Yuting Shi and Naoya Inoue and Houjing Wei and Yufeng Zhao and Tao Jin},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2024},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;url={https://aclanthology.org/2024.lrec-main.642/}<br>
    }