---
layout: page
permalink: /
title: "Hakaze Cho / Yufeng Zhao / 趙 羽風"
---

<div class="img_margin">
<img src="./assets/fig/photo.png" alt="" title="@Beijing Inst. Tech. 2023" height="220">
<figcaption>@Beijing Inst. Tech. 2023</figcaption>
</div>

**Ph.D. 2nd Year Student** @ [Graduate School of Information Science](https://www.jaist.ac.jp/areas/cs/){:target="_blank"}, [Japan Advanced Institute of Science and Technology](https://www.jaist.ac.jp/){:target="_blank"}  
**Fully-funded Research Assistant & Mentor** @ [RebelsNLU](https://rebelsnlu.super.site/){:target="_blank"}, PI: [A. Prof. Naoya Inoue](https://naoya-i.info/){:target="_blank"}   

**Alias**: Yufeng Zhao, both from the hieroglyph “趙 羽風”  
**Birth**: Beijing, 1999
<!-- **Affiliation**: Japan Advanced Institute of Science and Technology ← Beijing Institute of Technology   -->

**E-mail**: yfzhao [at] jaist.ac.jp  
**Links**:
[Twitter](https://x.com/yfZhao495){:target="_blank"} &nbsp;&nbsp;&nbsp;
[GitHub](https://github.com/hc495){:target="_blank"} &nbsp;&nbsp;&nbsp; 
[Google Scholar](https://scholar.google.com/citations?user=q_eQAcwAAAAJ){:target="_blank"} &nbsp;&nbsp;&nbsp; 
[ORCID](https://orcid.org/0000-0002-7127-1954){:target="_blank"} &nbsp;&nbsp;&nbsp; 
[Researchmap](https://researchmap.jp/hc495){:target="_blank"} &nbsp;&nbsp;&nbsp; 
[Semantic Scholar](https://www.semanticscholar.org/author/Hakaze-Cho/2304519017){:target="_blank"} &nbsp;&nbsp;&nbsp; 
[Blog](https://hakaze.notion.site/index?pvs=4){:target="_blank"} &nbsp;&nbsp;&nbsp;   
**Physical Address**: Laboratory I-52, Information Science Building I, 1-1 Asahidai, Nomi, Ishikawa, Japan  

I graduated from Beijing Institute of Technology, a top-ranking university in China, with a Master's degree in Software Engineering in 2023 and a Bachelor's degree in Chemistry in 2021. I am pursuing a Ph.D. at JAIST, with an expected early graduation in March 2026. My research focuses on exploring the internal mechanisms of artificial neural networks, particularly Transformer-based neural language models, during both training and inference by mathematical and representation learning methods, and improving their performance robustly through this deeper understanding. I have published over 20 papers / presentations in this area since 2023, some of which have been presented at top-tier international conferences such as ICLR and NAACL.

I am actively seeking productive research collaborations in the mentioned area. If you are interested in working together, please do not hesitate to contact me. I welcome collaborations with both experts and motivated beginners—being a novice is not a drawback if you are eager and efficient to learn. Additionally, I am open to exploring collaborations in other areas as well. 

[Japanese Site (日本語版)](https://www.hakaze-c.com/ja)

## Research Interests

**Keywords**: Representation Learning, Mechanistic Interpretability, In-context Learning  
- **Interpretability for Artificial Neural Network**: Mechanistic Interpretability, Low-resource Model Controlling  
- **Large Languages Models**: Mechanism of / Improving Transformer Large Language Models  
- **Misc.**: Manifold Learning, Low-precision Neural Networks, Neural Network Training Dynamics

## Publications

{% include_relative _includes/paper_statics.html %}

### International Conference

{% include_relative _includes/paper_list_international_c_papers.html %}

### Pre-print

{% include_relative _includes/paper_list_pre_print_papers.html %}

<!-- ### <a title="(† = Japan-domestic Secondary Publication for Conference Papers; Default: Non-refereed,▲= Refereed)">Domestic Conferences / Miscellaneous</a><br><span style="font-size:0.8em">(† = Japan-domestic Secondary Publication for International Conference Papers; Default: Non-refereed,▲= Refereed)</span> -->

### Domestic Conferences / Journal / Miscellaneous<br><span style="font-size:0.8em">(† = Japan-domestic Secondary Publication for International Conference Papers; Default: Non-refereed, ▲= Refereed)</span>

{% include_relative _includes/paper_list_domestic_c_papers_en.html %}


### (Thesis)

1. Fine-tuning with Randomly Initialized Downstream Network: Finding a Stable Convex-loss Region in Parameter Space    
    Yufeng Zhao   
    Master's Thesis - Rank A @ Beijing Institute of Technology. **2023**. 81 pages.
2. Synthesis and Self-Assembly of Aggregation-induced Emission Compounds   
   Yufeng Zhao   
   Bachelor Thesis @ Beijing Institute of Technology. **2021**. 52 pages.

## Resume

<div class="img_margin" style="display: flex; align-items: center; gap: 10px;">
    <img src="./assets/fig/jaist.png" height="105">
    <img src="./assets/fig/bit_xiaohui.jpg" height="150">
</div>

- **Ph.D.** in Computer Science, Research Assistant, 2023.10 ~ (2026.3)  
  [Graduate School of Information Science](https://www.jaist.ac.jp/areas/cs/){:target="_blank"}, [Japan Advanced Institute of Science and Technology](https://www.jaist.ac.jp/){:target="_blank"}  
  Mentor: [A. Prof. Naoya Inoue](https://naoya-i.info/){:target="_blank"}

- **M.Eng.** in Software Engeering, 2021.9 ~ 2023.6   
  [Graduate School of Computer Science and Technology](https://cs.bit.edu.cn/){:target="_blank"}, [Beijing Institute of Technology](https://www.bit.edu.cn/){:target="_blank"}   
  Mentor: Yufeng Zhao (Self-motivated)
  
- **B.Eng.** in Chemistry, 2017.8 ~ 2021.6  
  Department of Basic Science, [Beijing Institute of Technology](https://www.bit.edu.cn/){:target="_blank"}   
  Mentor: [A. Prof. Jianbing Shi](https://mse.bit.edu.cn/szdw/jgml/clwlyhxxg/ff4af2fd072b47beadc219b5c4e266f7.htm){:target="_blank"}

## Awards

- [Outstanding Paper](https://anlp.jp/nlp2025/award.html#outstanding){:target="_blank"} @ The 31st Annual Conference of the Japanese Association for Natural Language Processing (NLP2025, ANLP). 2025. (top 14 in 765, 2.0%)
- [Research Award for Young Scholars](https://sites.google.com/sig-nl.ipsj.or.jp/sig-nl/%E6%8E%88%E8%B3%9E/young#h.qq15e8v12s8d){:target="_blank"} @ The 260th SIG for Natural Language, Information Processing Society of Japan (SIG-NL260, IPSJ). 2024.
- [SB Intuitions Awards](https://www.anlp.jp/nlp2024/award.html){:target="_blank"} @ The 30st Annual Conference of the Japanese Association for Natural Language Processing (NLP2024, ANLP). 2024.
- Monbukagakusho Honors Scholarship @ Japanese Ministry of Education, Culture, Sports, Science and Technology. 2023.
- Outstanding Oral Presentation @ 2022 Euro-Asia Conference on Frontiers of Computer Science and Information Technology. 2022.
- GPA Improvement Award @ Beijing Institute of Technology. 2020. <small>I missed (medical) many exams in 2019, so my regular GPA in 2020 were considered a significant improvement.</small>
- Annual Outstanding Academic (GPA) Scholarship @ Beijing Institute of Technology. 2018, 2019, 2021, 2022, 2023.